{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310},{"sourceId":3594108,"sourceType":"datasetVersion","datasetId":2156255}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing libraries\n\nimport pandas as pd \nimport numpy as np\nfrom numpy import float32\nfrom tqdm import tqdm\n\nfrom typing import Optional\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import recall_score, confusion_matrix, precision_score,roc_auc_score,f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.multiprocessing import set_start_method\n\nimport pytorch_lightning as pl\n\nfrom torch.utils.data import DataLoader, Dataset\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","metadata":{"execution":{"iopub.status.busy":"2025-02-01T04:06:01.117627Z","iopub.execute_input":"2025-02-01T04:06:01.118031Z","execution_failed":"2025-02-01T04:06:04.373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_seed = 123\ntorch.manual_seed(random_seed)\n\nBATCH_SIZE=1000\nAVAIL_GPUS = min(1, torch.cuda.device_count())\nNUM_WORKERS=0\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df = pd.read_csv(\"/kaggle/input/credit-card-fraud/card_transdata.csv\")\ndf = pd.read_csv(\"/input/creditcardfraud/creditcard.csv\")\n#df.rename(columns={'fraud':'Class'},inplace=True)\n# df = df.drop(['type','nameDest','nameOrign'],axis=1)\nfrom sklearn.preprocessing import StandardScaler\nMx = df.iloc[:, :-1]  # 所有行，除了最后一列\nnx = df.iloc[:, -1]   # 所有行的最后一列\n\n# 创建 StandardScaler 对象\nscaler = StandardScaler()\n\n# 对特征进行拟合和转换\nMx_scaled = pd.DataFrame(scaler.fit_transform(Mx), columns=Mx.columns)\n\n# 如果你想保留原始的目标变量，可以这样做：\nscaled_data = pd.concat([Mx_scaled, nx], axis=1)\n\n# 现在，scaled_data 就是标准化后的数据集\n# 你可以查看结果\nscaled_data.head()\n","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.373Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating function to plot confusion metrics for evaluation\ndef plot_cm(labels, predictions, p=0.5):\n    cm = confusion_matrix(labels, predictions)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title('Confusion matrix @{:.2f}'.format(p))\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train, test = train_test_split(df, test_size=0.2, stratify=df['Class'])\ntrain.shape, test.shape","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalizing the data\ndata_mean = train.iloc[:,:-1].mean()\ndata_std = train.iloc[:,:-1].std()\ntrain_norm = (train.iloc[:,:-1] - data_mean)/data_std\ntest_norm = (test.iloc[:,:-1] - data_mean)/data_std\ntrain_norm['Class'] =  train.iloc[:, -1]\ntest_norm['Class'] =  test.iloc[:, -1]","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_norm['Class'].value_counts()","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.375Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Creating data model to use with the pytorch Lightning package\nclass CreditCardDataFinal(Dataset):\n    def __init__(self, data: pd.DataFrame):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        label = torch.tensor(self.data[index][-1], dtype=torch.float32)\n        row = torch.tensor(self.data[index][:-1], dtype=torch.float32)\n        return row, label\n    \nclass CreditCardDataModel(pl.LightningDataModule):\n    def __init__(self, data: pd.DataFrame, batch_size=BATCH_SIZE, num_workers=0):\n        super().__init__()\n        self.data = data\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n    \n    def setup(self, stage: Optional[str] = None):\n        train_df, test_df = train_test_split(self.data, random_state=123, test_size=0.2, stratify=self.data['Class'])\n        self.train_df = torch.tensor(train_norm.to_numpy(float32), dtype=torch.float32)\n        self.test_df = torch.tensor(test_norm.to_numpy(float32), dtype=torch.float32)\n\n    def train_dataloader(self):\n        return DataLoader(dataset=CreditCardDataFinal(self.train_df), batch_size=self.batch_size, num_workers=self.num_workers)\n    \n    def test_dataloader(self):\n        return DataLoader(dataset=CreditCardDataFinal(self.test_df), batch_size=self.batch_size, num_workers=self.num_workers)\n    ","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Building the main neural network to predict if the data is fraud or not. \nclass ModelCreditCard(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_size, 100),\n            nn.LeakyReLU(0.1),\n            nn.Linear(100, 50),\n            nn.LeakyReLU(0.1),\n            nn.Linear(50, 25),\n            nn.LeakyReLU(0.1),\n            nn.Linear(25, 1)\n        )\n    \n    def forward(self, x):\n        tensor = torch.sigmoid(self.sequential(x))\n        return tensor\n    \n\nclass ModelTraining(pl.LightningModule):\n    def __init__(self, model, lr=1e-3):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = model\n        \n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = torch.tensor(x, dtype=torch.float32)\n        y = torch.tensor(y, dtype=torch.float32)\n        y_hat = self.model(x)\n        y = y.unsqueeze(1)\n        loss = self.binary_loss(y_hat, y)\n        return {\"loss\":loss}\n    \n    def binary_loss(self, y_hat,y):\n        return F.binary_cross_entropy(y_hat, y)\n    \n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        opt_g = torch.optim.Adam(self.model.parameters(), lr )\n        return [opt_g], []","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=10, accelerator='cuda', devices=1)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.376Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card = ModelCreditCard(30)\nmodel = ModelTraining(model_card)\n\ndm = CreditCardDataModel(train_norm)\n\ntrainer = pl.Trainer(max_epochs=5, accelerator='gpu', devices=1)\ntrainer.fit(model, dm)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_pred = torch.tensor(test_norm.drop([\"Class\"], axis=1).to_numpy()).float()\ntest_true =  test_norm['Class'].to_numpy()\ntest_true = torch.tensor(test_true).unsqueeze(1).float()","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_output_real = model_card.forward(test_pred)\ntest_pred_real = [1 if i > 0.5 else 0 for i in test_output_real]\nrecall_score(test_true, test_pred_real)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CreditCardData(Dataset):\n    def __init__(self, data: pd.DataFrame):\n        self.data = data\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        row = torch.tensor(self.data.iloc[index].values).float()\n        return row","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DataModule(pl.LightningDataModule):\n    def __init__(self, data: pd.DataFrame, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n        super().__init__()\n        self.data = data\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.data_mean = None\n        self.data_std = None\n        \n    def prepare_data(self):\n        pass\n    \n    def setup(self, stage: Optional[str] = None):\n        train_df, test_df = train_test_split(self.data, random_state=123, test_size=0.2)\n        self.data_mean = train_df.mean()\n        self.data_std = train_df.std()\n        train_norm = (train_df - self.data_mean) / self.data_std\n        test_norm = (test_df - self.data_mean) / self.data_std\n        self.train_df = train_norm\n        self.test_df = test_norm\n    \n    def train_dataloader(self):\n        return DataLoader(dataset=CreditCardData(self.train_df), batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n    \n    def valid_dataloader(self):\n        return DataLoader(CreditCardData(self.val_df), batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n    \n    def test_dataloader(self):\n        return DataLoader(CreditCardData(self.test_df), batch_size=self.batch_size, num_workers=self.num_workers, pin_memory=True)\n        ","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.378Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GatedSelfAttention(nn.Module):\n    def __init__(self, input_dim):\n        super(GatedSelfAttention, self).__init__()\n        self.query = nn.Linear(input_dim, input_dim)\n        self.key = nn.Linear(input_dim, input_dim)\n        self.value = nn.Linear(input_dim, input_dim)\n        self.gate = nn.Linear(input_dim, input_dim)  # 新增的门控层\n\n    def forward(self, x):\n        Q = self.query(x)\n        K = self.key(x)\n        V = self.value(x)\n        G = torch.sigmoid(self.gate(x))  # 计算门控信号，并通过Sigmoid激活\n        \n        # 计算注意力权重\n        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / (x.size(-1) ** 0.5)\n        attention_weights = F.softmax(attention_scores, dim=-1)\n        \n        # 应用门控机制到值向量上\n        gated_V = V * G  # 逐元素乘法\n        \n        # 应用注意力权重到门控后的值向量\n        attention_output = torch.bmm(attention_weights, gated_V)\n        \n        return attention_output","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.379Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, input_dim=30):\n        super(Discriminator, self).__init__()\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv1d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.LeakyReLU(0.2)\n        )\n\n        self.attention = GatedSelfAttention(64)  # 使用新的门控自注意力模块\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(64 * input_dim, 128),\n            nn.LeakyReLU(0.2),\n            nn.Linear(128, 64),\n            nn.LeakyReLU(0.2),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, x):\n        x = x.unsqueeze(1)\n        x = self.conv_layers(x)\n        x = x.transpose(1, 2)\n        x = self.attention(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layers(x)\n        return torch.sigmoid(x)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-01T04:06:04.379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, latent_dim, output_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.output_dim = output_dim\n\n        # 编码器部分：将输入数据映射到潜在空间参数\n        self.encoder = nn.Sequential(\n            nn.Linear(output_dim, 100),\n            nn.LeakyReLU(0.2),\n            nn.Linear(100, 100),\n            nn.LeakyReLU(0.2)\n        )\n        self.fc_mu = nn.Linear(100, latent_dim)        # 均值向量\n        self.fc_logvar = nn.Linear(100, latent_dim)    # 对数方差向量\n\n        # 解码器部分（生成器）：从潜在空间重建数据\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 100),\n            nn.LeakyReLU(0.2),\n            nn.Linear(100, 100),\n            nn.LeakyReLU(0.2),\n            nn.Linear(100, output_dim)\n        )\n\n    def reparameterize(self, mu, log_var):\n        \"\"\"重参数化技巧\"\"\"\n        std = torch.exp(0.5 * log_var)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        # 编码过程\n        h = self.encoder(x)\n        mu = self.fc_mu(h)\n        log_var = self.fc_logvar(h)\n        \n        # 重参数化采样\n        z = self.reparameterize(mu, log_var)\n        \n        # 解码重建\n        x_recon = self.decoder(z)\n        return x_recon, mu, log_var\n\n    def sample(self, num_samples, device):\n        \"\"\"从潜在空间生成样本\"\"\"\n        z = torch.randn(num_samples, self.latent_dim).to(device)\n        samples = self.decoder(z)\n        return samples\n#classic generator\n#class Generator(nn.Module):\n    #def __init__(self, latent_dim, output_dim):\n        #super().__init__()\n        #self.latent_dim = latent_dim\n        #self.output_dim = output_dim\n        \n        #self.model = nn.Sequential(\n            #nn.Linear(latent_dim, output_dim)\n            #nn.Linear(latent_dim, 100),\n            #nn.LeakyReLU(0.2),\n            #nn.Linear(100, 100),\n            #nn.LeakyReLU(0.2),\n            #nn.Linear(100, output_dim)\n        #)\n\n    #def forward(self, z):\n        #return self.model(z)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.379Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GAN(pl.LightningModule):\n    def __init__(self, latent_dim=100, lr=0.002):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        self.generator = Generator(latent_dim=self.hparams.latent_dim,output_dim=30)  # 假设输出维度为28)\n        self.discriminator = Discriminator()\n        \n        self.validation_z = torch.randn(6, self.hparams.latent_dim).to(self.device)\n\n        self.automatic_optimization = False  # Disable automatic optimization\n        \n    def setup(self, stage):\n        # 获取数据模块\n        data_module = self.trainer.datamodule\n        \n        # 获取数据统计信息\n        self.data_mean = torch.tensor(data_module.data_mean.values, dtype=torch.float32)\n        self.data_std = torch.tensor(data_module.data_std.values, dtype=torch.float32)\n\n    def forward(self, z):\n        # 生成标准化的数据\n        x_normalized = self.generator(z)\n        # 使用数据的均值和方差进行反标准化\n        #x = x_normalized * self.data_std + self.data_mean\n        x = x_normalized\n        return x\n    \n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n    \n    def training_step(self, batch, batch_idx):\n        real_data = batch #.to(self.device)\n        z = torch.randn(real_data.size(0), self.hparams.latent_dim).to(self.device)\n        \n        # Access optimizers manually\n        opt_g, opt_d = self.optimizers()\n        \n        # Train Complementary GAN:\n        \n        # Train CCFD-generator: max log(D(G(z)))\n        fake_data = self.generator(z)\n        y_hat_fake = self.discriminator(fake_data)\n        g_loss = self.adversarial_loss(y_hat_fake, torch.ones_like(y_hat_fake))\n        \n        opt_g.zero_grad()\n        self.manual_backward(g_loss)\n        opt_g.step()\n        \n        # Train CCFD-discriminator: max log(D(x)) + log(1- D(G(z)))\n        y_hat_real = self.discriminator(real_data)\n        y_real = torch.ones(real_data.size(0), 1).to(self.device)\n        real_loss = self.adversarial_loss(y_hat_real, y_real)\n        \n        y_hat_fake = self.discriminator(fake_data.detach())\n        y_fake = torch.zeros(real_data.size(0), 1).to(self.device)\n        fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n        \n        d_loss = (real_loss + fake_loss) / 2\n        \n        opt_d.zero_grad()\n        self.manual_backward(d_loss)\n        opt_d.step()\n        \n        self.log(\"g_loss\", g_loss, prog_bar=True, logger=True)\n        self.log(\"d_loss\", d_loss, prog_bar=True, logger=True)\n        \n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr)\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr)\n        return [opt_g, opt_d]","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.380Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"only_pos = train_norm[train_norm['Class'] == 1 ].copy()\nonly_pos = only_pos.drop(['Class'], axis=1)\nonly_pos.shape","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.380Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GAN()\nmodel = model.to(device)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.380Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dm = DataModule(only_pos)\ntrainer = pl.Trainer(max_epochs=500)\ntrainer.fit(model, dm)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z = torch.randn(227451,100)\n#227451 730078\noutput = model(z)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"z = torch.randn(10000,100,16)\nt = torch.randint(0, model.hparams.timesteps, (100, 1), dtype=torch.float, device=model.device)\noutput = model(z, t)","metadata":{}},{"cell_type":"code","source":"output","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"only_pos_df =  pd.DataFrame(output.detach().numpy())\nonly_pos_df","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"only_pos_df['Class'] = 1","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"only_neg_real_df = train_norm[train_norm['Class'] == 0]","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.383Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_combined_fake_pos_only = only_pos_df\ntrain_combined_fake_pos_only.columns = only_neg_real_df.columns\ntrain_combined_fake_pos_only = pd.concat([only_pos_df, only_neg_real_df], ignore_index=True)\ntrain_combined_fake_pos_only = train_combined_fake_pos_only.sample(frac=1) # Shuffle the dataset","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_combined_fake_pos_only['Class'].value_counts()","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_card_with_gan = ModelCreditCard(30)\nmodel_gan = ModelTraining(model_card_with_gan)\ndm_only_gan = CreditCardDataModel(train_combined_fake_pos_only)\ntrainer = pl.Trainer(max_epochs=10, accelerator='cuda', devices=1)\ntrainer.fit(model_gan, dm_only_gan)","metadata":{"execution":{"execution_failed":"2025-02-01T04:06:04.384Z"},"trusted":true},"outputs":[],"execution_count":null}]}